{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c6e8104",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b10b0677",
   "metadata": {},
   "source": [
    "### Text Cleaning and Sampling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8539804",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success. Saved 15,000 rows to ../Data/filtered/sampled_complaints.csv\n",
      "Product\n",
      "Debt collection                                            5091\n",
      "Checking or savings account                                4251\n",
      "Money transfer, virtual currency, or money service         2944\n",
      "Credit card                                                2444\n",
      "Payday loan, title loan, personal loan, or advance loan     270\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "input_file = '../Data/filtered/filtered_complaints.csv'\n",
    "output_file = '../Data/filtered/sampled_complaints.csv'\n",
    "\n",
    "def clean_narrative(text):\n",
    "    if not isinstance(text, str): return \"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'i am writing to file a complaint|to whom it may concern|dear cfpb|x{2,}', '', text)\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s.,!?]', '', text)\n",
    "    return \" \".join(text.split())\n",
    "\n",
    "#Identify valid products (>= 10 rows)\n",
    "chunk_iter = pd.read_csv(input_file, chunksize=100000, usecols=['Product'])\n",
    "all_counts = pd.Series(dtype=int)\n",
    "for chunk in chunk_iter:\n",
    "    all_counts = all_counts.add(chunk['Product'].value_counts(), fill_value=0)\n",
    "\n",
    "valid_products = all_counts[all_counts >= 10].index.tolist()\n",
    "\n",
    "# Process, clean, and collect all eligible rows\n",
    "processed_data = []\n",
    "for chunk in pd.read_csv(input_file, chunksize=50000):\n",
    "    chunk = chunk[chunk['Product'].isin(valid_products)].copy()\n",
    "    chunk['cleaned_narrative'] = chunk['Consumer complaint narrative'].apply(clean_narrative)\n",
    "    processed_data.append(chunk)\n",
    "\n",
    "df_all = pd.concat(processed_data)\n",
    "\n",
    "# Stratified Sampling\n",
    "df_sample, _ = train_test_split(\n",
    "    df_all, \n",
    "    train_size=15000, \n",
    "    stratify=df_all['Product'], \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "df_sample.to_csv(output_file, index=False)\n",
    "print(f\"Success. Saved 15,000 rows to {output_file}\")\n",
    "print(df_sample['Product'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be77d21",
   "metadata": {},
   "source": [
    "### Chunking , Embedding and storing into a vector store  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba9cf652",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Length: 24630 characters\n",
      "Total Chunks Created: 57\n",
      "\n",
      " CHUNK 1 ---\n",
      "the county court is a corporation. judge was acting as an administrator and not listening or properly reviewing the court record. objected multiple times that lawyer was abusing the legal process, since already met the burden of proof with several certified letters sent out withing 5 days of recieving notice from the parties working with the judge committing fraud and , then , . judge did not have consent showing up in court the day , in special appearance as a live man. reserved their rights without prejudice ucc 1308\n",
      "\n",
      " CHUNK 2 (Look for the overlap at the start) ---\n",
      ". reserved their rights without prejudice ucc 1308. also reserved their given rights, common law rights, civil rights and constitutional protections. they also invoked common law and described with legal citations that that matter was blended with law and common law equity, and cited ucc 103.6. the judge was unfamiliar with blended law and the citations of v. , and v. , he did not know about any more recent cases that the case was based on being retired since . explained about v. since there were fraudulent assignments out of a irs static trust, ie tax evasion\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Grab the longest narrative from sample to test the \"worst case\"\n",
    "test_text = df_sample.loc[df_sample['cleaned_narrative'].str.len().idxmax(), 'cleaned_narrative']\n",
    "print(f\"Original Length: {len(test_text)} characters\")\n",
    "\n",
    "# Set up the experiment\n",
    "chunk_size = 600\n",
    "chunk_overlap = 100\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=chunk_overlap,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
    ")\n",
    "\n",
    "# Perform the split\n",
    "test_chunks = splitter.split_text(test_text)\n",
    "\n",
    "print(f\"Total Chunks Created: {len(test_chunks)}\\n\")\n",
    "\n",
    "# Inspect the first two chunks to see the \"Overlap\"\n",
    "print(\" CHUNK 1 ---\")\n",
    "print(test_chunks[0])\n",
    "print(\"\\n CHUNK 2 (Look for the overlap at the start) ---\")\n",
    "print(test_chunks[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78602dba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Length: 24630 characters\n",
      "New Total Chunks: 32\n",
      "Sample Chunk:\n",
      ". the judge was unfamiliar with blended law and the citations of v. , and v. , he did not know about any more recent cases that the case was based on being retired since . explained about v. since there were fraudulent assignments out of a irs static trust, ie tax evasion. the trust was set up confo\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "test_text = df_sample.loc[df_sample['cleaned_narrative'].str.len().idxmax(), 'cleaned_narrative']\n",
    "print(f\"Original Length: {len(test_text)} characters\")\n",
    "\n",
    "# Set up the experiment\n",
    "chunk_size = 600\n",
    "chunk_overlap = 100\n",
    "\n",
    "# Larger chunks to reduce 70 fragments to something more manageable\n",
    "bigger_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,   # Increased from 600\n",
    "    chunk_overlap=200, # Increased to keep 2-3 sentences of context\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
    ")\n",
    "\n",
    "bigger_chunks = bigger_splitter.split_text(test_text)\n",
    "print(f\"New Total Chunks: {len(bigger_chunks)}\")\n",
    "print(f\"Sample Chunk:\\n{bigger_chunks[1][:300]}\") # Show the start of the next piece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7a6a68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9009317",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
